{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_response = youtube.videos().list(\n",
    "            part=\"snippet\",\n",
    "            id=\"tQxpdY8sWHM\"\n",
    "        ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets video comments using the youtube api\n",
    "def get_comments(video_ids: list, n_comments: int, n_buffer_comments: int = 0, lang_code: str = 'en'):\n",
    "\n",
    "    video_comments = {}\n",
    "\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            comments_response = youtube.commentThreads().list(\n",
    "                part=\"snippet\", \n",
    "                videoId=video_id, \n",
    "                maxResults=n_comments + n_buffer_comments, \n",
    "                order=\"relevance\"\n",
    "            ).execute() # Fetch comments\n",
    "\n",
    "            comments = []\n",
    "            response_comments = [item['snippet']['topLevelComment']['snippet']['textDisplay'] for item in comments_response[\"items\"]]\n",
    "\n",
    "            for comment in response_comments:\n",
    "                try:\n",
    "                    if ld.detect(comment) == lang_code and len(comments) < n_comments:\n",
    "                        comments.append(comment)\n",
    "                except ld.LangDetectException:\n",
    "                    # Handle the case where the language detection fails\n",
    "                    pass\n",
    "\n",
    "            video_comments[video_id] = comments\n",
    "\n",
    "        except googleapiclient.errors.HttpError as e:\n",
    "            print(f\"Error processing video {video_id}: {e}\")\n",
    "\n",
    "    return video_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sentiment_analyzer(analyzer: str, comments: list, relevant_words:list, sentiment_pipeline = None):\n",
    "\n",
    "    labeled_comments = collections.defaultdict(list)\n",
    "    \n",
    "    for comment in comments:\n",
    "        if is_relevant_comment(comment, relevant_words):\n",
    "            match analyzer:\n",
    "                case 'text_blob':\n",
    "                    #print(\"Comment outside text_blob function:    \", comment)\n",
    "                    polarity = TextBlob(comment).sentiment.polarity\n",
    "                    polarity = POSITIVE if polarity > 0 else NEGATIVE if polarity < 0 else NEUTRAL\n",
    "                case 'vader':\n",
    "                    raise Exception(\"Vader analyzer not implemented yet\")\n",
    "                case 'bert':\n",
    "                    #print(\"Comment outside bert function:         \", comment)\n",
    "                    polarity = bert(comment, sentiment_pipeline, neutral_threshold=0.55)\n",
    "                case _:\n",
    "                    raise Exception(\"Invalid or no analyzer provided\")\n",
    "        else:\n",
    "            polarity = NEUTRAL\n",
    "\n",
    "        labeled_comments[polarity].append({\n",
    "            'text': comment,\n",
    "            'polarity': polarity\n",
    "        })\n",
    "\n",
    "    # Limit comments to top 30 per polarity\n",
    "    for polarity in labeled_comments:\n",
    "        labeled_comments[polarity] = labeled_comments[polarity][:30]\n",
    "\n",
    "    # Flatten the dictionaries into lists\n",
    "    labeled_comments_flat = [comment for comments in labeled_comments.values() for comment in comments]\n",
    "\n",
    "    # print(labeled_comments_flat)\n",
    "\n",
    "    return labeled_comments_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_comments = read_video_comments(\"video_comments.json\")[\"ro130m-f_yk\"]\n",
    "relevant_words = [\"ai\", \"controversy\", \"jobs\", \"cars\", \"music\", \"art\", \"programming\", \"languages\"]\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "labeled_comments = run_sentiment_analyzer(\"bert\", video_comments, relevant_words, sentiment_pipeline)\n",
    "df = pd.DataFrame(labeled_comments)\n",
    "df[\"polarity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_comments = read_video_comments(\"video_comments.json\")\n",
    "for video_id in video_comments:\n",
    "    print(video_id, len(video_comments[video_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sentiment_analyzer(analyzer: str, comments: list, relevant_words:list, sentiment_pipeline = None):\n",
    "\n",
    "    labeled_comments = []\n",
    "    \n",
    "    for comment in comments:\n",
    "        match analyzer:\n",
    "            case 'text_blob':\n",
    "                #print(\"Comment outside text_blob function:    \", comment)\n",
    "                polarity = TextBlob(comment).sentiment.polarity\n",
    "                polarity = POSITIVE if polarity > 0 else NEGATIVE if polarity < 0 else NEUTRAL\n",
    "            case 'vader':\n",
    "                raise Exception(\"Vader analyzer not implemented yet\")\n",
    "            case 'bert':\n",
    "                #print(\"Comment outside bert function:         \", comment)\n",
    "                polarity = bert(comment, sentiment_pipeline, neutral_threshold=0.55)\n",
    "            case _:\n",
    "                raise Exception(\"Invalid or no analyzer provided\")\n",
    "\n",
    "        labeled_comments.append({\n",
    "            'text': comment,\n",
    "            'polarity': polarity\n",
    "        })\n",
    "\n",
    "    return labeled_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words to include\n",
    "# topics = [\"ai controversy\", \"ai jobs controversy\", \"ai cars controversy\", \"ai music controversy\", \"ai art controversy\", \"ai programming controversy\"]\n",
    "topics = [\"ai controversy\", \"ai jobs controversy\", \"ai cars controversy\", \"ai music controversy\", \"ai art controversy\", \"ai programming controversy\"]\n",
    "relevant_words = [\"ai\", \"controversy\", \"jobs\", \"cars\", \"music\", \"art\", \"programming\", \"languages\"]\n",
    "\n",
    "# Extract videos on topic\n",
    "if (os.path.exists(\"video_ids.json\")):\n",
    "    print(\"Reading video ids from file\")\n",
    "    video_ids = read_video_ids(\"video_ids.json\")\n",
    "else:\n",
    "    print(\"Searching for videos using the API\")\n",
    "    video_ids = get_video_ids(topics, 5, 5)\n",
    "    write_video_ids(video_ids, \"video_ids.json\")\n",
    "\n",
    "\n",
    "video_ids = sum(read_video_ids(\"video_ids.json\").values(), [])\n",
    "\n",
    "# Retrieve comments for each video\n",
    "if (os.path.exists(\"video_comments.json\")):\n",
    "    print(\"Reading comments from file\")\n",
    "    video_comments = read_video_comments(\"video_comments.json\")\n",
    "else:\n",
    "    print(\"Retrieving comments using the API\")\n",
    "    video_comments = get_comments(video_ids, 250)\n",
    "    write_video_comments(video_comments, \"video_comments.json\")\n",
    "\n",
    "all_comments_TB = []\n",
    "all_comments_BERT = []\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "for comment_list in video_comments.values():\n",
    "    all_comments_TB.extend(run_sentiment_analyzer(\"text_blob\", comment_list, relevant_words))\n",
    "    all_comments_BERT.extend(run_sentiment_analyzer(\"bert\", comment_list, relevant_words, sentiment_pipeline))\n",
    "\n",
    "df_TB = pd.DataFrame(all_comments_TB)\n",
    "df_BERT = pd.DataFrame(all_comments_BERT)\n",
    "\n",
    "print(df_TB.head())\n",
    "print(df_BERT.head())\n",
    "\n",
    "df_TB.to_csv('TB_comments.csv', index=False)\n",
    "df_BERT.to_csv('BERT_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devesh's Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Comments = 30*3*6*5 = 2700\n",
    "\n",
    "# List of words to include\n",
    "list_of_words = [\"ai\", \"controversy\", \"jobs\", \"cars\", \"music\", \"art\", \"programming\", \"languages\"]\n",
    "topics = [\"ai controversy\", \"ai jobs controversy\", \"ai cars controversy\", \"ai music controversy\", \"ai art controversy\", \"ai programming controversy\"]\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "def get_sentiment(comment, sentiment_pipeline, neutral_threshold=0.55): # Default threshold is 0.55\n",
    "    # Truncate comments to avoid exceeding the model's maximum token length\n",
    "    comment_truncated = comment[:512] \n",
    "    result = sentiment_pipeline(comment_truncated)[0]\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "\n",
    "    # Determine sentiment based on score threshold\n",
    "    if score < neutral_threshold:\n",
    "        polarity = 0  # Neutral\n",
    "    else:\n",
    "        polarity = 1 if label == 'POSITIVE' else -1\n",
    "\n",
    "    return polarity\n",
    "\n",
    "def fetch_video_details_and_comments(video_id): # Fetch video details and comments for a given video ID.\n",
    "    video_data_TB = collections.defaultdict(list)\n",
    "    video_data_bert = collections.defaultdict(list)\n",
    "\n",
    "    try:\n",
    "        comments_response = youtube.commentThreads().list(part=\"snippet\", videoId=video_id, maxResults=250).execute() # Fetch comments\n",
    "\n",
    "        # Fetch comments and their polarities.\n",
    "        for item in comments_response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            if any(re.search(r'\\b' + re.escape(word) + r'\\b', comment, re.IGNORECASE) for word in list_of_words):\n",
    "                try:\n",
    "                    # Check if the comment is in English\n",
    "                    if ld.detect(comment) == 'en':\n",
    "                        # TextBlob polarity\n",
    "                        polarity_TB = TextBlob(comment).sentiment.polarity\n",
    "                        polarity_TB = 1 if polarity_TB > 0 else (-1 if polarity_TB < 0 else 0)\n",
    "                        video_data_TB[polarity_TB].append({\n",
    "                            'text': comment,\n",
    "                            'polarity': polarity_TB\n",
    "                        })\n",
    "\n",
    "                        # BERT polarity\n",
    "                        polarity_bert = get_sentiment(comment, sentiment_pipeline)\n",
    "                        print(polarity_bert)\n",
    "                        video_data_bert[polarity_bert].append({\n",
    "                            'text': comment,\n",
    "                            'polarity': polarity_bert\n",
    "                        })          \n",
    "                except ld.LangDetectException:\n",
    "                    # Handle the case where the language detection fails\n",
    "                    pass  \n",
    "            elif any(re.search(r'\\b' + re.escape(word) + r'\\b', comment, re.IGNORECASE) is None for word in list_of_words):\n",
    "                try:\n",
    "                    # Check if the comment is in English\n",
    "                    if ld.detect(comment) == 'en':\n",
    "                        # TextBlob polarity\n",
    "                        polarity_TB = 0\n",
    "                        video_data_TB[polarity_TB].append({\n",
    "                            'text': comment,\n",
    "                            'polarity': polarity_TB\n",
    "                        })\n",
    "\n",
    "                        # BERT polarity\n",
    "                        polarity_bert = 0\n",
    "                        video_data_bert[polarity_bert].append({\n",
    "                            'text': comment,\n",
    "                            'polarity': polarity_bert\n",
    "                        })          \n",
    "                except ld.LangDetectException:\n",
    "                    # Handle the case where the language detection fails\n",
    "                    pass  \n",
    "    \n",
    "    except googleapiclient.errors.HttpError as e:\n",
    "        print(f\"Error processing video {video_id}: {e}\")\n",
    "\n",
    "    # Limit comments to top 30 per polarity for TextBlob and BERT\n",
    "    for polarity in video_data_TB:\n",
    "        video_data_TB[polarity] = video_data_TB[polarity][:30] # Limit to 30 comments per polarity\n",
    "    for polarity in video_data_bert:\n",
    "        video_data_bert[polarity] = video_data_bert[polarity][:30] \n",
    "\n",
    "    # Flatten the dictionaries into lists\n",
    "    video_data_TB_flat = [comment for comments in video_data_TB.values() for comment in comments]\n",
    "    video_data_bert_flat = [comment for comments in video_data_bert.values() for comment in comments]\n",
    "\n",
    "    return video_data_TB_flat, video_data_bert_flat\n",
    "\n",
    "def main():\n",
    "\n",
    "    if (len(read_video_ids(\"video_ids.json\")) == 6): #If we already have saved the videos, we can just read them.\n",
    "        video_ids = read_video_ids(\"video_ids.json\")\n",
    "        for i in video_ids:\n",
    "            print(i, \":\", video_ids[i])\n",
    "    else: #Otherwise, we need to fetch the videos and save them.\n",
    "        video_ids = get_video_ids(topics, 5, 5)\n",
    "        write_video_ids(video_ids, \"video_ids.json\")\n",
    "        for i in video_ids:\n",
    "            print(i, \":\", video_ids[i])\n",
    "    \n",
    "    all_data_TB = [] \n",
    "    all_data_bert = [] \n",
    "    for topic,ids in video_ids.items():\n",
    "        for video_id in ids:\n",
    "            video_data_TB, video_data_bert  = fetch_video_details_and_comments(video_id)\n",
    "            all_data_TB.extend(video_data_TB) # Add video data to the list.\n",
    "            all_data_bert.extend(video_data_bert) # Add video data to the list.\n",
    "\n",
    "    df_TB = pd.DataFrame(all_data_TB)\n",
    "    df_bert = pd.DataFrame(all_data_bert)\n",
    "\n",
    "    print(df_TB.head())\n",
    "    print(df_bert.head())\n",
    "    df_TB.to_csv('TB_comments.csv', index=False)\n",
    "    df_bert.to_csv('BERT_comments.csv', index=False)\n",
    "\n",
    "main()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model : Pipeline, X_train : list, X_test : list, y_train : list, y_test : list):\n",
    "    class_values = [-1, 0, 1]\n",
    "    class_names = [\"Negative\", \"Neutral \", \"Positive\"]\n",
    "    model_name = model.named_steps['classifier'].__class__.__name__\n",
    "    y_pred_train, y_pred_test, y_proba = run_model(model, X_train, X_test, y_train, y_test)\n",
    "    train_score = accuracy_score(y_train, y_pred_train)\n",
    "    test_score = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "    # Binarize the labels for multi-label/multi-class ROC\n",
    "    y_test_bin = label_binarize(y_test, classes = class_values)\n",
    "    y_pred_bin = label_binarize(y_pred_test, classes = class_values)\n",
    "\n",
    "    # Print out train and test accuracies\n",
    "    print(\"Train score:\", train_score)\n",
    "    print(\"Test Score:\", test_score)\n",
    "    \n",
    "    # Call functions for evaluation\n",
    "    display_cf_report(y_test, y_pred_test)\n",
    "    plot_confusion_matrix(y_test, y_pred_test, class_names)\n",
    "    list_TP_FP(y_test_bin, y_pred_bin, class_names)\n",
    "    # plot_roc_curve(y_test_bin, y_proba, class_names, f\"ROC for {model_name}\", f\"Macro-Average ROC for {model_name}\")\n",
    "\n",
    "    return test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifiers(models, X_train, y_train, X_test, y_test):\n",
    "    log_cols=[\"Classifier\", \"Train Accuracy\", \"Test Accuracy\"]\n",
    "    log = pd.DataFrame(columns=log_cols)\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        score_train = accuracy_score(y_train, y_pred_train)\n",
    "        print(\"Accuracy of {} with train dataset is: {:0.3f}\".format(model, score_train))\n",
    "        \n",
    "        y_pred_test = model.predict(X_test)\n",
    "        score_test = accuracy_score(y_test, y_pred_test)\n",
    "        print(\"Accuracy of {} with test dataset is: {:0.3f}\".format(model, score_test))\n",
    "    \n",
    "        log_entry = pd.DataFrame([[model.named_steps['classifier'].__class__.__name__, score_train*100, score_test*100]], columns=log_cols)\n",
    "        log = pd.concat([log, log_entry])\n",
    "    \n",
    "    compare_metrics_graph(log)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
